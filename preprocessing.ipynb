{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-12-01T13:30:42.076424Z",
     "iopub.status.busy": "2021-12-01T13:30:42.07601Z",
     "iopub.status.idle": "2021-12-01T13:30:46.133713Z",
     "shell.execute_reply": "2021-12-01T13:30:46.132781Z",
     "shell.execute_reply.started": "2021-12-01T13:30:42.076331Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "import pattern\n",
    "from pattern.en import lemma\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-01T13:30:46.135469Z",
     "iopub.status.busy": "2021-12-01T13:30:46.13509Z",
     "iopub.status.idle": "2021-12-01T13:31:31.508993Z",
     "shell.execute_reply": "2021-12-01T13:31:31.508308Z",
     "shell.execute_reply.started": "2021-12-01T13:30:46.135437Z"
    }
   },
   "outputs": [],
   "source": [
    "df1=pd.read_csv('../input/data-for-codeproject-contest/our_contest_train_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-01T13:31:31.513304Z",
     "iopub.status.busy": "2021-12-01T13:31:31.513052Z",
     "iopub.status.idle": "2021-12-01T13:31:31.546648Z",
     "shell.execute_reply": "2021-12-01T13:31:31.545835Z",
     "shell.execute_reply.started": "2021-12-01T13:31:31.51327Z"
    }
   },
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-01T13:31:31.556116Z",
     "iopub.status.busy": "2021-12-01T13:31:31.555895Z",
     "iopub.status.idle": "2021-12-01T13:31:31.561277Z",
     "shell.execute_reply": "2021-12-01T13:31:31.560519Z",
     "shell.execute_reply.started": "2021-12-01T13:31:31.55609Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "def lemmatize_text(sentence):\n",
    "    return \" \".join([lemma(wd) for wd in sentence.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heading column preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-01T13:31:31.562852Z",
     "iopub.status.busy": "2021-12-01T13:31:31.562626Z",
     "iopub.status.idle": "2021-12-01T13:32:15.3668Z",
     "shell.execute_reply": "2021-12-01T13:32:15.365846Z",
     "shell.execute_reply.started": "2021-12-01T13:31:31.562826Z"
    }
   },
   "outputs": [],
   "source": [
    "df1['Heading']=df1.Heading.apply(gensim.utils.simple_preprocess)\n",
    "df1['Heading']=df1['Heading'].apply(' '.join)\n",
    "df1['Heading']=df1.Heading.apply(gensim.parsing.preprocessing.remove_stopwords)\n",
    "df1['Heading']=df1.Heading.apply(gensim.parsing.preprocessing.strip_multiple_whitespaces)\n",
    "df1['Heading']=df1.Heading.apply(gensim.parsing.preprocessing.strip_non_alphanum)\n",
    "df1['Heading']=df1.Heading.apply(gensim.parsing.preprocessing.strip_numeric)\n",
    "df1['Heading']=df1.Heading.apply(gensim.parsing.preprocessing.strip_punctuation)\n",
    "df1['Heading']=df1.Heading.apply(gensim.parsing.preprocessing.strip_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-01T13:33:02.716411Z",
     "iopub.status.busy": "2021-12-01T13:33:02.715975Z",
     "iopub.status.idle": "2021-12-01T13:33:14.199432Z",
     "shell.execute_reply": "2021-12-01T13:33:14.198497Z",
     "shell.execute_reply.started": "2021-12-01T13:33:02.71637Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"NOTE : if the above code raises an error saying ‘generator raised StopIteration’.\n",
    "Just run it again. It will work after 3-4 tries.\"\"\"\n",
    "df1.loc[:,'Heading'] = df1.loc[:,'Heading'].astype('str')\n",
    "df1.loc[:,'Heading'] =df1.Heading.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-01T13:34:08.506919Z",
     "iopub.status.busy": "2021-12-01T13:34:08.506517Z",
     "iopub.status.idle": "2021-12-01T13:34:08.522948Z",
     "shell.execute_reply": "2021-12-01T13:34:08.522149Z",
     "shell.execute_reply.started": "2021-12-01T13:34:08.50689Z"
    }
   },
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"heading_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"../input/preprocessing/heading_processed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MainText column preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dropna(subset=['MainText'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1[\"MainText\"].apply(lambda x: isinstance(x, float))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['MainText'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1['MainText']=df1.MainText.apply(gensim.utils.simple_preprocess)\n",
    "df1['MainText']=df1['MainText'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['MainText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.to_csv(\"maintext_processed0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1['MainText']=df1['MainText'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.to_csv(\"maintext_processed1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['MainText']=df1.MainText.apply(gensim.parsing.preprocessing.remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"maintext_processed2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['MainText']=df1.MainText.apply(gensim.parsing.preprocessing.strip_multiple_whitespaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"maintext_processed3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['MainText']=df1.MainText.apply(gensim.parsing.preprocessing.strip_non_alphanum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"maintext_processed4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['MainText']=df1.MainText.apply(gensim.parsing.preprocessing.strip_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"maintext_processed5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['MainText']=df1.MainText.apply(gensim.parsing.preprocessing.strip_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"maintext_processed6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['MainText']=df1.MainText.apply(gensim.parsing.preprocessing.strip_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"maintext_processed7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['MainText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[:,'MainText'] = df1.loc[:,'MainText'].astype('str')\n",
    "df1.loc[:,'MainText'] =df1.MainText.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['MainText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"maintext_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"../input/preprocessing/maintext_processed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subject preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Subject'] = df1[['PrimarySubject', 'SecondarySubject', 'TertiarySubject','OtherSubject']].apply(lambda x: ','.join(x.dropna()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[:,'Subject'] = df1.loc[:,'Subject'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.drop(columns=['PrimarySubject', 'SecondarySubject', 'TertiarySubject','OtherSubject'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['Subject'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1[\"Subject\"].apply(lambda x: isinstance(x, float))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dropna(subset=['Subject'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"Subject\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop(df1.columns[[0, 1]],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Subject']=df1.Subject.apply(gensim.utils.simple_preprocess)\n",
    "df1['Subject']=df1['Subject'].apply(' '.join)\n",
    "df1['Subject']=df1.Subject.apply(gensim.parsing.preprocessing.remove_stopwords)\n",
    "df1['Subject']=df1.Subject.apply(gensim.parsing.preprocessing.strip_multiple_whitespaces)\n",
    "df1['Subject']=df1.Subject.apply(gensim.parsing.preprocessing.strip_non_alphanum)\n",
    "df1['Subject']=df1.Subject.apply(gensim.parsing.preprocessing.strip_numeric)\n",
    "df1['Subject']=df1.Subject.apply(gensim.parsing.preprocessing.strip_punctuation)\n",
    "df1['Subject']=df1.Subject.apply(gensim.parsing.preprocessing.strip_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[:,'Subject'] = df1.loc[:,'Subject'].astype('str')\n",
    "df1.loc[:,'Subject'] =df1.Subject.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"subject_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['PostDateTime'] = pd.to_datetime(df1['PostDateTime'],infer_datetime_format=True)\n",
    "df1['WhenAccountMade'] = pd.to_datetime(df1['WhenAccountMade'],infer_datetime_format=True)\n",
    "df1['Diff_time']=(df1['PostDateTime']-df1['WhenAccountMade'])/ np.timedelta64(1, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df1[['Karma','NumAnswers','Diff_time']]=scaler.fit_transform(df1[['Karma','NumAnswers','Diff_time']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop(['PostDateTime', 'WhenAccountMade', 'Qid'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"time_processing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1.isna().any(axis=1)].Target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"final_preprocessing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T14:00:18.266475Z",
     "iopub.status.busy": "2021-11-30T14:00:18.266174Z",
     "iopub.status.idle": "2021-11-30T14:00:46.898139Z",
     "shell.execute_reply": "2021-11-30T14:00:46.89728Z",
     "shell.execute_reply.started": "2021-11-30T14:00:18.266435Z"
    }
   },
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"../input/preprocessing/final_preprocessing.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T13:33:03.65821Z",
     "iopub.status.busy": "2021-11-30T13:33:03.658005Z",
     "iopub.status.idle": "2021-11-30T13:33:03.668201Z",
     "shell.execute_reply": "2021-11-30T13:33:03.667381Z",
     "shell.execute_reply.started": "2021-11-30T13:33:03.658185Z"
    }
   },
   "outputs": [],
   "source": [
    "#gensim is very popular model to learn word2vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# size=int - Dimensionality of the feature vectors. - (50,300)\n",
    "# min_count = int - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
    "def avg_word_2_vec(df,col,size_vec,cores,minimum_count):\n",
    "  preprocessed_text=[] \n",
    "  for sentance in tqdm(df[col].values):\n",
    "    preprocessed_text.append(sentance.strip())\n",
    "\n",
    "  list_of_sentance=[]\n",
    "  for sentance in tqdm(preprocessed_text):\n",
    "    list_of_sentance.append(sentance.split())\n",
    "\n",
    "  w2v_model=Word2Vec(list_of_sentance,min_count=minimum_count,vector_size=size_vec, workers=cores)\n",
    "  w2v_words = list(w2v_model.wv.key_to_index)\n",
    "  print(\"number of words that occured minimum 5 times \",len(w2v_words))\n",
    "  print(\"sample words \", w2v_words[0:50])\n",
    "\n",
    "  # average Word2Vec\n",
    "  # compute average word2vec for each review.\n",
    "  sent_vectors = []; # the avg-w2v for each sentence is stored in this list\n",
    "  for sent in tqdm(list_of_sentance): # for each sentence\n",
    "    sent_vec = np.zeros(size_vec) \n",
    "    cnt_words =0; # num of words with a valid vector in the sentence\n",
    "    for word in sent: # for each word in a sentence\n",
    "      if word in w2v_words:\n",
    "        vec = w2v_model.wv[word]\n",
    "        sent_vec += vec\n",
    "        cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "      sent_vec /= cnt_words\n",
    "    sent_vectors.append(sent_vec)\n",
    "  print(len(sent_vectors))\n",
    "  print(len(sent_vectors[0]))\n",
    "  return sent_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T12:59:01.588053Z",
     "iopub.status.busy": "2021-11-30T12:59:01.58783Z",
     "iopub.status.idle": "2021-11-30T12:59:01.600222Z",
     "shell.execute_reply": "2021-11-30T12:59:01.599565Z",
     "shell.execute_reply.started": "2021-11-30T12:59:01.588028Z"
    }
   },
   "outputs": [],
   "source": [
    "#gensim is very popular model to learn word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# size=int - Dimensionality of the feature vectors. - (50,300)\n",
    "# min_count = int - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
    "def tfidf_word_2_vec(df,col,size_vec,cores,minimum_count):\n",
    "  preprocessed_text=[] \n",
    "  for sentance in tqdm(df[col].values):\n",
    "    preprocessed_text.append(sentance.strip())\n",
    "\n",
    "  list_of_sentance=[]\n",
    "  for sentance in tqdm(preprocessed_text):\n",
    "    list_of_sentance.append(sentance.split())\n",
    "\n",
    "  w2v_model=Word2Vec(list_of_sentance,min_count=minimum_count,vector_size=size_vec, workers=cores)\n",
    "  w2v_words = list(w2v_model.wv.key_to_index)\n",
    "  print(\"number of words that occured minimum 5 times \",len(w2v_words))\n",
    "  print(\"sample words \", w2v_words[0:50])\n",
    "\n",
    "  model = TfidfVectorizer()\n",
    "  model.fit(preprocessed_text)\n",
    "  # we are cat\n",
    "verting a dictionary with word as a key, and the idf as a value\n",
    "  dictionary = dict(zip(model.get_feature_names(), list(model.idf_)))\n",
    "  \n",
    "  # TF-IDF weighted Word2Vec\n",
    "  tfidf_feat = model.get_feature_names() # tfidf words/col-names\n",
    "\n",
    "  tfidf_sent_vectors = []; # the tfidf-w2v for each sentence is stored in this list\n",
    "  for sent in tqdm(list_of_sentance): # for each sentence \n",
    "    sent_vec = np.zeros(size_vec) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "      if word in w2v_words and word in tfidf_feat:\n",
    "        vec = w2v_model.wv[word]\n",
    "        # dictionary[word] = idf value of word in whole courpus\n",
    "        # sent.count(word) = tf valeus of word in this text\n",
    "        tf_idf = dictionary[word]*(sent.count(word)/len(sent))\n",
    "        sent_vec += (vec * tf_idf)\n",
    "        weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "      sent_vec /= weight_sum\n",
    "    tfidf_sent_vectors.append(sent_vec)\n",
    "  print(len(tfidf_sent_vectors))\n",
    "  print(len(tfidf_sent_vectors[0]))\n",
    "  return tfidf_sent_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-30T13:16:19.558584Z",
     "iopub.status.idle": "2021-11-30T13:16:19.559064Z",
     "shell.execute_reply": "2021-11-30T13:16:19.558839Z",
     "shell.execute_reply.started": "2021-11-30T13:16:19.558815Z"
    }
   },
   "outputs": [],
   "source": [
    "Heading_vect_tfidf=tfidf_word_2_vec(df1,'Heading',50,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-30T13:16:19.560129Z",
     "iopub.status.idle": "2021-11-30T13:16:19.560575Z",
     "shell.execute_reply": "2021-11-30T13:16:19.560356Z",
     "shell.execute_reply.started": "2021-11-30T13:16:19.560332Z"
    }
   },
   "outputs": [],
   "source": [
    "Heading_vect_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-30T13:16:19.561842Z",
     "iopub.status.idle": "2021-11-30T13:16:19.562309Z",
     "shell.execute_reply": "2021-11-30T13:16:19.562082Z",
     "shell.execute_reply.started": "2021-11-30T13:16:19.562049Z"
    }
   },
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(Heading_vect_tfidf) \n",
    "dataframe.to_csv(\"Heading_vect_tfidf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T14:01:56.252178Z",
     "iopub.status.busy": "2021-11-30T14:01:56.251838Z"
    }
   },
   "outputs": [],
   "source": [
    "#MainText_vect_tfidf=tfidf_word_2_vec(df1,'MainText',50,4,5)\n",
    "#MainText_vect_tfidf=avg_word_2_vec(df1,'MainText',50,4,5)\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), \n",
    "                                   min_df = 2, \n",
    "                                   max_df = .95)\n",
    "MainText_vect_tfidf = tfidf_vectorizer.fit_transform(df1['MainText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD(n_components=50, \n",
    "                   n_iter=10, \n",
    "                   random_state=3)\n",
    "\n",
    "MainText_vect_tfidf = tscd.fit_transform(MainText_vect_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MainText_vect_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(MainText_vect_tfidf) \n",
    "dataframe.to_csv(\"MainText_vect_tfidf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Subject_vect_tfidf=tfidf_word_2_vec(df1,'Subject',50,4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Subject_vect_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(Subject_vect_tfidf) \n",
    "dataframe.to_csv(\"Subject_vect_tfidf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
